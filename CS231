{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8940125,"sourceType":"datasetVersion","datasetId":5379258}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hoanghungnguynkhnh/cs231?scriptVersionId=248444058\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nimport cv2 as cv\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nimport torchvision\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom torchvision.models import resnet50, ResNet50_Weights \n\nfrom PIL import Image \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:51:22.537549Z","iopub.execute_input":"2025-07-02T06:51:22.538195Z","iopub.status.idle":"2025-07-02T06:51:31.071716Z","shell.execute_reply.started":"2025-07-02T06:51:22.538159Z","shell.execute_reply":"2025-07-02T06:51:31.071116Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/mri-for-brain-tumor-with-bounding-boxes'\nTRAIN_PATH = os.path.join(BASE_PATH, 'Train')\nTEST_PATH = os.path.join(BASE_PATH, 'Val')\n\ntry: \n    CLASS_NAMES = sorted([d for d in os.listdir(TRAIN_PATH) if os.path.isdir(os.path.join(TRAIN_PATH))])\nexcept FileNotFoundError:\n    print(f\"Error: TRAIN_PATH '{TRAIN_PATH}' not found. Please check the path.\")\n    CLASS_NAMES = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n\nprint(f\"Discovered class names: {CLASS_NAMES}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:51:31.072852Z","iopub.execute_input":"2025-07-02T06:51:31.073358Z","iopub.status.idle":"2025-07-02T06:51:31.082874Z","shell.execute_reply.started":"2025-07-02T06:51:31.07333Z","shell.execute_reply":"2025-07-02T06:51:31.082332Z"}},"outputs":[{"name":"stdout","text":"Discovered class names: ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"IMG_SIZE = (224,224)\nBATCH_SIZE = 128\nNUM_EPOCHS = 100 \nLEARNING_RATE = 5e-4\nLAMBDA_BBOX = 10.0 \nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:51:31.083463Z","iopub.execute_input":"2025-07-02T06:51:31.083686Z","iopub.status.idle":"2025-07-02T06:51:31.157888Z","shell.execute_reply.started":"2025-07-02T06:51:31.08365Z","shell.execute_reply":"2025-07-02T06:51:31.157061Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def get_image_and_label_paths_for_dataset(base_path, classes_list):\n    \"\"\" \n    Collects image paths, corresponding label file paths, and class labels \n    Filters out images with missing or invalid label files\n    \"\"\"\n\n    all_images_paths = []\n    all_label_file_paths = []\n    all_class_labels = []\n\n    for cls in classes_list:\n        img_dir =  os.path.join(base_path, cls, 'images')\n        label_dir = os.path.join(base_path, cls, 'labels')\n\n        if not os.path.exists(img_dir):\n            print(f\"Warning: Image directory not found for class {cls} at {img_dir}\")\n            continue\n\n        if not os.path.exists(label_dir):\n            print(f\"Warning: Label directory not found for class {cls} at {label_dir}\")\n            continue\n\n        for fname in os.listdir(img_dir):\n            if fname.lower().endswith(('.jpg', '.jpeg', '.png', 'bmp')):\n                image_path = os.path.join(img_dir, fname)\n                label_name = os.path.splitext(fname)[0] + '.txt'\n                label_path = os.path.join(label_dir, label_name)\n\n                if os.path.exists(label_path) and os.path.getsize(label_path) > 0: \n                    try: \n                        with open(label_path, 'r') as f: \n                            line = f.readline().strip().split()\n                            if len(line) == 5: \n                                all_images_paths.append(image_path)\n                                all_label_file_paths.append(label_path)\n                                all_class_labels.append(cls)\n                            else: print(f\"Warning: invalid label format in {label_path} for image path {image_path}. Skipping\")\n\n                    except Exception as e:\n                        print(f\"Warning: Could not read or parse label file {label_path} for image {image_path}. Error: {e}. Skipping.\")\n                else:\n                    print(f\"Warning: Label file missing or empty for {image_path}. Skipping.\") \n\n    return all_images_paths, all_label_file_paths, all_class_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:51:31.159365Z","iopub.execute_input":"2025-07-02T06:51:31.159589Z","iopub.status.idle":"2025-07-02T06:51:31.177228Z","shell.execute_reply.started":"2025-07-02T06:51:31.159572Z","shell.execute_reply":"2025-07-02T06:51:31.176601Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class Brain_Tumor_Dataset(Dataset):\n    \"\"\" \n    Custom Dataset for Brain Tumor images and bounding boxes. \n    \"\"\"\n\n    def __init__(self, image_paths, label_file_paths, class_indices, transform=None):\n        self.images_paths = image_paths\n        self.label_file_paths = label_file_paths\n        self.class_indicies = class_indices\n        self.transform = transform\n\n\n    def __len__(self):\n        return len(self.images_paths)\n    \n    def __getitem__(self, index):\n        image = Image.open(self.images_paths[index]).convert(\"RGB\")\n        class_label = self.class_indicies[index]\n        bbox = torch.tensor([0.0, 0.0, 0.0, 0.0], dtype=torch.float32)\n        current_label_file_path = self.label_file_paths[index]\n\n        if current_label_file_path is not None: \n            try: \n                with open(current_label_file_path, 'r') as f: \n                    line = f.readline().strip().split()\n                    if len(line) == 5: \n                        bbox_coords = list(map(float, line[1:]))\n                        bbox = torch.tensor(bbox_coords, dtype=torch.float32)\n                    else: \n                        print(f\"Warning: Re-encountering invalid bbox in file during getitem: {current_label_file_path}. Using dummy bbox\")\n\n            except Exception as e: \n                print(f\"Warning: Error reading bbox file {current_label_file_path} during getitem: {e}. Using dummy bbox\")\n\n        if self.transform: \n            image = self.transform(image)\n\n        class_label_tensor = torch.tensor(class_label, dtype=torch.long)\n        return image, class_label_tensor, bbox \n    \n\ndef prepare_DataLoaders(data_path, classes_list, image_size, batch_size, test_split_ratio=0.2, random_state=42):\n    \"\"\" \n    Prepares training and validation DataLoader \n    \"\"\"\n    image_paths, label_file_paths, string_labels = get_image_and_label_paths_for_dataset(data_path, classes_list)\n        \n    if not image_paths: \n        raise ValueError(\"No valid images found. Please check dataset paths and label files.\")\n        \n    label_encoder = LabelEncoder() \n    encoded_numeric_labels = label_encoder.fit_transform(string_labels)\n\n        # Train and validation dataset split \n\n    train_img_paths, val_img_paths, \\\n    train_label_files, val_label_files, \\\n    train_numeric_labels, val_numeric_labels = train_test_split(image_paths, label_file_paths, encoded_numeric_labels,\n                                                                    test_size=test_split_ratio, random_state=random_state,\n                                                                    stratify=encoded_numeric_labels)\n    train_transform = transforms.Compose([\n        transforms.Resize(image_size), \n        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n        transforms.RandomRotation(15), \n        transforms.ToTensor(), \n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet stats\n        ])\n\n    val_transform = transforms.Compose([\n        transforms.Resize(image_size), \n        transforms.ToTensor(), \n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet stats\n    ])\n\n    train_dataset = Brain_Tumor_Dataset(train_img_paths, train_label_files, train_numeric_labels, transform=train_transform)\n    val_dataset = Brain_Tumor_Dataset(val_img_paths, val_label_files, val_numeric_labels, transform=val_transform )\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    return train_loader, val_loader, label_encoder\n    \n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:51:31.177988Z","iopub.execute_input":"2025-07-02T06:51:31.178184Z","iopub.status.idle":"2025-07-02T06:51:31.194001Z","shell.execute_reply.started":"2025-07-02T06:51:31.178168Z","shell.execute_reply":"2025-07-02T06:51:31.193405Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def prepare_Test_DataLoaders(data_path, classes_list, image_size, batch_size, random_state=42):\n    \"\"\" \n    Prepares test DataLoader \n    \"\"\"\n    test_image_paths, test_label_file_paths, test_string_labels = get_image_and_label_paths_for_dataset(data_path, classes_list)\n        \n    if not test_image_paths: \n        raise ValueError(\"No valid images found. Please check dataset paths and label files.\")\n        \n    label_encoder = LabelEncoder() \n    encoded_numeric_labels = label_encoder.fit_transform(test_string_labels)\n\n    test_transform = transforms.Compose([\n        transforms.Resize(image_size), \n        transforms.ToTensor(), \n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n    ])\n\n    print()\n    test_dataset = Brain_Tumor_Dataset(test_image_paths, test_label_file_paths, encoded_numeric_labels, transform=test_transform)\n\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    return test_loader, label_encoder\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:51:31.194864Z","iopub.execute_input":"2025-07-02T06:51:31.195136Z","iopub.status.idle":"2025-07-02T06:51:31.214105Z","shell.execute_reply.started":"2025-07-02T06:51:31.19512Z","shell.execute_reply":"2025-07-02T06:51:31.213506Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_loader, val_loader, label_encoder = prepare_DataLoaders(TRAIN_PATH, CLASS_NAMES, IMG_SIZE, BATCH_SIZE) \nprint(f\"\\nDataLoaders Prepared:\")\nprint(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\nprint(f\"Number of classes: {len(label_encoder.classes_)}\")\nprint(f\"Class: {dict(enumerate(label_encoder.classes_))}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:51:31.214837Z","iopub.execute_input":"2025-07-02T06:51:31.215073Z","iopub.status.idle":"2025-07-02T06:52:05.183767Z","shell.execute_reply.started":"2025-07-02T06:51:31.215047Z","shell.execute_reply":"2025-07-02T06:52:05.183129Z"}},"outputs":[{"name":"stdout","text":"\nDataLoaders Prepared:\nTrain batches: 30, Validation batches: 8\nNumber of classes: 4\nClass: {0: 'Glioma', 1: 'Meningioma', 2: 'No Tumor', 3: 'Pituitary'}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"test_loader, label_encoder = prepare_Test_DataLoaders(TEST_PATH, CLASS_NAMES, IMG_SIZE, BATCH_SIZE)\nprint(f\"\\nTest DataLoaders Prepared:\")\nprint(f\"Test batches: {len(test_loader)}\")\nprint(f\"Number of classes: {len(label_encoder.classes_)}\")\nprint(f\"Class: {dict(enumerate(label_encoder.classes_))}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:52:05.184437Z","iopub.execute_input":"2025-07-02T06:52:05.18466Z","iopub.status.idle":"2025-07-02T06:52:09.725761Z","shell.execute_reply.started":"2025-07-02T06:52:05.184642Z","shell.execute_reply":"2025-07-02T06:52:09.725022Z"}},"outputs":[{"name":"stdout","text":"Warning: Label file missing or empty for /kaggle/input/mri-for-brain-tumor-with-bounding-boxes/Val/Glioma/images/gg (342).jpg. Skipping.\nWarning: Label file missing or empty for /kaggle/input/mri-for-brain-tumor-with-bounding-boxes/Val/No Tumor/images/image(61).jpg. Skipping.\nWarning: Label file missing or empty for /kaggle/input/mri-for-brain-tumor-with-bounding-boxes/Val/No Tumor/images/image(55).jpg. Skipping.\n\n\nTest DataLoaders Prepared:\nTest batches: 4\nNumber of classes: 4\nClass: {0: 'Glioma', 1: 'Meningioma', 2: 'No Tumor', 3: 'Pituitary'}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class MultiHeadedResNet(nn.Module):\n    def __init__(self, num_classes, pretrained=True):\n        super(MultiHeadedResNet, self).__init__()\n\n        # Load pretrained ResNet50 \n        weights = ResNet50_Weights.DEFAULT if pretrained else None \n        resnet = resnet50(weights=weights)\n\n        # BackBone: All layers of ResNet50 except the original average poolong and fully connected layer \n        self.backbone = nn.Sequential(*list(resnet.children())[:-2]) \n        num_backbone_feature = resnet.fc.in_features \n\n        # Classification head \n        self.classification_head = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)), \n            nn.Flatten(), \n            nn.Linear(num_backbone_feature, 512), \n            nn.ReLU(), \n            nn.BatchNorm1d(512),  # Added batchNorm\n            nn.Dropout(0.2), \n            nn.Linear(512, num_classes)\n        )\n\n        # Localization_head \n        self.localization_head = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)), \n            nn.Flatten(), \n            nn.Linear(num_backbone_feature, 512), \n            nn.ReLU(), \n            nn.BatchNorm1d(512), \n            nn.Linear(512, 4), \n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        features = self.backbone(x) \n        class_logits = self.classification_head(features) \n        bbox_predictions = self.localization_head(features)\n        return class_logits, bbox_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:52:09.726606Z","iopub.execute_input":"2025-07-02T06:52:09.726854Z","iopub.status.idle":"2025-07-02T06:52:09.73291Z","shell.execute_reply.started":"2025-07-02T06:52:09.726836Z","shell.execute_reply":"2025-07-02T06:52:09.732308Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Generalized IoU loss - Modified to accept YOLO format and apply reduction \nclass GIoULoss(nn.Module):\n    def __init__(self, eps=1e-7, reduction='mean') :\n        \"\"\" \n        Args: \n            eps (float): A small value to avoid divisble by zero \n            reduction (str): Specifies the reduction to apply to the ouput \n                    'none' | 'mean' | 'sum'\n                    \n                    'mean': batch mean loss\n                    'sum': batch sum loss\n                    'none': no reduction \n        \"\"\"\n\n        super().__init__() \n        self.eps = eps \n        if reduction not in ['none', 'mean', 'sum']:\n            raise ValueError(f\"Reduction method: '{reduction}' not supported. Choose from 'none', 'mean', 'sum'.\")\n        self.reduction = reduction\n\n\n    def forward(self, pred_bbox, target_bbox):\n        \"\"\" \n        Calculate the GIoU loss. \n        Args: \n            - pred_bbox (torch.Tensor): Predicted bounding boxes (c_x, c_y, w, h). Shape (batch_size, 4).\n            Return: \n            torch.Tensor: the calculated loss. Scalar if reduction is 'mean' or 'sum', tensor of shape (batch_size) \n            if reduction is 'none' \n        \"\"\"\n\n\n        # Format input is YOLO but the GIoU loss only works on COCO format \n        # Assume input format YOLO format. Ensure tensors have the same shape \n        if pred_bbox.shape != target_bbox.shape: \n            raise ValueError(\"Predicted and target bounding boxes must have the same shape\")\n        if pred_bbox.shape[-1] != 4: \n            raise ValueError(\"Bounding box tensors must have size 4 in the last dimension (cx, cy, w, h)\")\n        \n        # Use unbind(-1) for safety and clear \n        cx_pred, cy_pred, w_pred, h_pred = pred_bbox.unbind(-1)\n        cx_true, cy_true, w_true, h_true = target_bbox.unbind(-1)\n\n        # Conver (cx, cy, w, h) to (x1, y1, x2, y2)\n        \n        # Min coordinates \n        xmin_pred = cx_pred - w_pred / 2\n        ymin_pred = cy_pred - h_pred / 2\n\n        xmin_true = cx_true - w_true / 2\n        ymin_true = cy_true - h_true / 2\n\n        # Max coordinates \n        xmax_pred = cx_pred + w_pred / 2\n        ymax_pred = cy_pred + h_pred / 2\n        \n        xmax_true = cx_true + w_true / 2\n        ymax_true = cy_true + h_true / 2\n\n        # --------- GIoU calculation ------------\n        # Calculate intersection coordinates\n        intersection_x1 = torch.max(xmin_pred, xmin_true)\n        intersection_y1 = torch.max(ymin_pred, ymin_true)\n        intersection_x2 = torch.min(xmax_pred, xmax_true)\n        intersection_y2 = torch.min(ymax_pred, ymax_true)\n\n\n        # Calculate intersection area \n        # Use clamp(min=0) in case the boxes do not overlap (width or height becomes negative)\n        intersection = torch.clamp(intersection_x2 - intersection_x1, min=0) * torch.clamp(intersection_y2 - intersection_y1, min=0)\n\n        # Predicted and target areas \n        # The output has shape \n        pred_area = w_pred * h_pred\n        true_area = h_true * w_true\n\n        # Calculate Union area \n        union = pred_area + true_area - intersection + self.eps \n\n        # Calculate IoU \n        iou = intersection / union \n\n        # Calculate the smallest enclosing box coordinates \n        enclose_x1 = torch.min(xmin_pred, xmin_true)\n        enclose_y1 = torch.min(ymin_pred, ymin_true)\n        enclose_x2 = torch.max(xmax_pred, xmax_true)\n        enclose_y2 = torch.max(ymax_pred, ymax_true)\n\n        # Calculate enclosing box area \n        enclose_area = torch.clamp(enclose_x2 - enclose_x1, min=0) * torch.clamp(enclose_y2 - enclose_y1, min=0) + self.eps\n\n        # Calculate GIoU \n        giou = iou - (enclose_area - union) / enclose_area \n\n        \n        # The loss for each box is 1 - GIoU \n\n        loss = 1 - giou \n\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum() \n        elif self.reduction == 'none':\n            return loss\n        else: \n            raise ValueError(f\"Reduction method '{self.reduction}' not supported\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:52:09.735043Z","iopub.execute_input":"2025-07-02T06:52:09.735302Z","iopub.status.idle":"2025-07-02T06:52:09.754097Z","shell.execute_reply.started":"2025-07-02T06:52:09.735286Z","shell.execute_reply":"2025-07-02T06:52:09.753571Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def train_model_epoch(model, dataloader, class_criterion, bbox_criterion, optimizer, device, lambda_bbox_weight):\n    model.train() \n    total_loss = 0.0 \n    total_class_loss = 0.0 \n    total_bbox_loss = 0.0 \n\n    # For accuracy calculation \n    correct_classification = 0 \n    total_samples = 0 \n\n    for images, class_labels, target_bboxes in dataloader: \n        images = images.to(device)\n        class_labels = class_labels.to(device)\n        target_bboxes = target_bboxes.to(device)\n\n        \n        class_logits, pred_bboxes = model(images) \n        loss_classification = class_criterion(class_logits, class_labels)\n        loss_bbox = bbox_criterion(pred_bboxes, target_bboxes)\n\n        combined_loss = loss_classification + lambda_bbox_weight * loss_bbox \n\n        combined_loss.backward() \n        optimizer.step() \n\n        total_loss += combined_loss.item() \n        total_class_loss += loss_classification.item()\n        total_bbox_loss += loss_bbox.item() \n\n        _, predicted_classes = torch.max(class_logits, 1)\n\n        correct_classification += (predicted_classes == class_labels).sum().item() \n\n        total_samples += class_labels.size(0)\n\n    avg_loss = total_loss / len(dataloader)\n    avg_class_loss = total_class_loss / len(dataloader)\n    avg_bbob_loss = total_bbox_loss / len(dataloader) \n    accuracy = (correct_classification / total_samples) * 100 if total_samples > 0 else 0 \n\n    return avg_loss, avg_class_loss, avg_bbob_loss, accuracy\n\n\ndef evaluate_model_epoch(model, dataloader, class_criterion, bbox_criterion, device, lambda_bbox_weight):\n    model.eval()\n    total_loss = 0.0 \n    total_class_loss = 0.0 \n    total_bbox_loss = 0.0 \n\n    correct_classification = 0 \n    total_samples = 0\n\n    with torch.no_grad(): \n        for images, class_labels, target_bboxes in dataloader: \n            images = images.to(device)\n            class_labels = class_labels.to(device)\n            target_bboxes = target_bboxes.to(device)\n\n            class_logits, pred_bboxes = model(images)\n            loss_classification = class_criterion(class_logits, class_labels)\n            loss_bboxes = bbox_criterion(pred_bboxes, target_bboxes)\n\n            combined_loss = loss_classification + lambda_bbox_weight*loss_bboxes\n\n            total_loss += combined_loss.item() \n            total_class_loss += loss_classification.item()\n            total_bbox_loss += loss_bboxes.item()\n\n            _, predicted_classes = torch.max(class_logits, 1)\n\n            correct_classification += (predicted_classes == class_labels).sum().item() \n            total_samples += class_labels.size(0)\n\n        avg_loss = total_loss / len(dataloader)\n        avg_class_loss = total_class_loss / len(dataloader)\n        avg_bbox_loss = total_bbox_loss / len(dataloader)\n        accuracy = (correct_classification / total_samples) * 100 if total_samples > 0 else 0\n\n    return avg_loss, avg_class_loss, avg_bbox_loss, accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:52:09.754705Z","iopub.execute_input":"2025-07-02T06:52:09.754903Z","iopub.status.idle":"2025-07-02T06:52:09.77643Z","shell.execute_reply.started":"2025-07-02T06:52:09.754889Z","shell.execute_reply":"2025-07-02T06:52:09.77588Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Initialize model\nnum_model_classes = len(label_encoder.classes_)\nmodel = MultiHeadedResNet(num_classes=num_model_classes, pretrained=True)\nmodel = model.to(DEVICE)\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:52:09.777216Z","iopub.execute_input":"2025-07-02T06:52:09.777508Z","iopub.status.idle":"2025-07-02T06:52:11.108759Z","shell.execute_reply.started":"2025-07-02T06:52:09.777493Z","shell.execute_reply":"2025-07-02T06:52:11.108054Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 186MB/s]\n","output_type":"stream"},{"name":"stdout","text":"MultiHeadedResNet(\n  (backbone): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (5): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (6): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (7): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n  )\n  (classification_head): Sequential(\n    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Linear(in_features=2048, out_features=512, bias=True)\n    (3): ReLU()\n    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): Dropout(p=0.2, inplace=False)\n    (6): Linear(in_features=512, out_features=4, bias=True)\n  )\n  (localization_head): Sequential(\n    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Linear(in_features=2048, out_features=512, bias=True)\n    (3): ReLU()\n    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): Linear(in_features=512, out_features=4, bias=True)\n    (6): Sigmoid()\n  )\n)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"criterion_class = nn.CrossEntropyLoss()\ncriterion_bbox = GIoULoss(eps=1e-7)\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:52:11.109256Z","iopub.execute_input":"2025-07-02T06:52:11.109462Z","iopub.status.idle":"2025-07-02T06:52:11.116036Z","shell.execute_reply.started":"2025-07-02T06:52:11.109447Z","shell.execute_reply":"2025-07-02T06:52:11.115511Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(\"\\nStarting Model Training...\")\nhistory = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [],\n           'train_class_loss': [], 'val_class_loss': [],\n           'train_bbox_loss': [], 'val_bbox_loss': []}\n\n# best_val_loss = float('inf')\n# patience_counter = 0\n# max_patience = 7 # For early stopping\n\nfor epoch in range(NUM_EPOCHS):\n    train_loss, train_c_loss, train_b_loss, train_acc = train_model_epoch(\n        model, train_loader, criterion_class, criterion_bbox, optimizer, DEVICE, LAMBDA_BBOX\n    )\n    val_loss, val_c_loss, val_b_loss, val_acc = evaluate_model_epoch(\n        model, val_loader, criterion_class, criterion_bbox, DEVICE, LAMBDA_BBOX\n    )\n\n    history['train_loss'].append(train_loss)\n    history['train_class_loss'].append(train_c_loss)\n    history['train_bbox_loss'].append(train_b_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_class_loss'].append(val_c_loss)\n    history['val_bbox_loss'].append(val_b_loss)\n    history['val_acc'].append(val_acc)\n\n    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n    print(f\"  Train -> Loss: {train_loss:.4f} (Cls: {train_c_loss:.4f}, BBox: {train_b_loss:.4f}), Acc: {train_acc:.2f}%\")\n    print(f\"  Valid -> Loss: {val_loss:.4f} (Cls: {val_c_loss:.4f}, BBox: {val_b_loss:.4f}), Acc: {val_acc:.2f}%\")\n\n    scheduler.step(val_loss)\n\n    # if val_loss < best_val_loss:\n    #     best_val_loss = val_loss\n    #     torch.save(model.state_dict(), 'best_model_state.pth')\n    #     print(f\"  Saved best model state (Val Loss: {best_val_loss:.4f})\")\n    #     patience_counter = 0\n    # else:\n    #     patience_counter += 1\n    #     if patience_counter >= max_patience:\n    #         print(f\"Early stopping triggered after {max_patience} epochs without improvement.\")\n    #         break\n            \nprint(\"\\nTraining Finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T06:52:11.116796Z","iopub.execute_input":"2025-07-02T06:52:11.117059Z","execution_failed":"2025-07-02T07:12:45.906Z"}},"outputs":[{"name":"stdout","text":"\nStarting Model Training...\nEpoch [1/100]\n  Train -> Loss: 10.5458 (Cls: 0.7136, BBox: 0.9832), Acc: 72.18%\n  Valid -> Loss: 133.6656 (Cls: 115.8477, BBox: 1.7818), Acc: 14.98%\nEpoch [2/100]\n  Train -> Loss: 9.2808 (Cls: 0.6087, BBox: 0.8672), Acc: 77.51%\n  Valid -> Loss: 25.9782 (Cls: 12.4628, BBox: 1.3515), Acc: 25.74%\nEpoch [3/100]\n  Train -> Loss: 9.4936 (Cls: 0.7973, BBox: 0.8696), Acc: 71.81%\n  Valid -> Loss: 17.8501 (Cls: 7.7647, BBox: 1.0085), Acc: 25.53%\nEpoch [4/100]\n  Train -> Loss: 9.5403 (Cls: 0.7989, BBox: 0.8741), Acc: 73.21%\n  Valid -> Loss: 45.2765 (Cls: 32.0467, BBox: 1.3230), Acc: 14.98%\nEpoch [5/100]\n  Train -> Loss: 9.9841 (Cls: 1.1522, BBox: 0.8832), Acc: 66.88%\n  Valid -> Loss: 25.6249 (Cls: 13.9488, BBox: 1.1676), Acc: 30.49%\nEpoch [6/100]\n  Train -> Loss: 10.3468 (Cls: 1.3726, BBox: 0.8974), Acc: 57.75%\n  Valid -> Loss: 36.7753 (Cls: 26.7793, BBox: 0.9996), Acc: 15.72%\nEpoch [7/100]\n  Train -> Loss: 10.6687 (Cls: 1.6133, BBox: 0.9055), Acc: 54.66%\n  Valid -> Loss: 704.5811 (Cls: 691.5061, BBox: 1.3075), Acc: 22.05%\nEpoch [8/100]\n  Train -> Loss: 10.6072 (Cls: 1.5481, BBox: 0.9059), Acc: 57.96%\n  Valid -> Loss: 139.1501 (Cls: 128.9846, BBox: 1.0166), Acc: 33.54%\nEpoch [9/100]\n  Train -> Loss: 10.1862 (Cls: 1.1512, BBox: 0.9035), Acc: 58.64%\n  Valid -> Loss: 67.7192 (Cls: 57.9720, BBox: 0.9747), Acc: 32.49%\nEpoch [10/100]\n  Train -> Loss: 10.2834 (Cls: 1.2711, BBox: 0.9012), Acc: 63.08%\n  Valid -> Loss: 537.3926 (Cls: 527.4924, BBox: 0.9900), Acc: 46.20%\nEpoch [11/100]\n  Train -> Loss: 10.6342 (Cls: 1.6198, BBox: 0.9014), Acc: 60.73%\n  Valid -> Loss: 242.6196 (Cls: 232.9576, BBox: 0.9662), Acc: 65.40%\nEpoch [12/100]\n  Train -> Loss: 10.5436 (Cls: 1.4155, BBox: 0.9128), Acc: 64.37%\n  Valid -> Loss: 43.8394 (Cls: 34.5374, BBox: 0.9302), Acc: 68.99%\nEpoch [13/100]\n  Train -> Loss: 10.1883 (Cls: 1.1610, BBox: 0.9027), Acc: 66.88%\n  Valid -> Loss: 186.9746 (Cls: 177.6987, BBox: 0.9276), Acc: 70.68%\nEpoch [14/100]\n  Train -> Loss: 9.8491 (Cls: 0.9198, BBox: 0.8929), Acc: 68.28%\n  Valid -> Loss: 160.0502 (Cls: 150.8899, BBox: 0.9160), Acc: 72.05%\nEpoch [15/100]\n  Train -> Loss: 10.0926 (Cls: 1.0021, BBox: 0.9091), Acc: 69.41%\n  Valid -> Loss: 209.9985 (Cls: 200.8591, BBox: 0.9139), Acc: 72.57%\nEpoch [16/100]\n  Train -> Loss: 9.9198 (Cls: 0.9680, BBox: 0.8952), Acc: 71.02%\n  Valid -> Loss: 575.7929 (Cls: 566.8997, BBox: 0.8893), Acc: 75.00%\nEpoch [17/100]\n  Train -> Loss: 9.7033 (Cls: 0.9259, BBox: 0.8777), Acc: 74.03%\n  Valid -> Loss: 280.4603 (Cls: 271.6991, BBox: 0.8761), Acc: 74.68%\nEpoch [18/100]\n  Train -> Loss: 9.7696 (Cls: 0.9151, BBox: 0.8854), Acc: 74.06%\n  Valid -> Loss: 479.5979 (Cls: 470.5956, BBox: 0.9002), Acc: 75.53%\nEpoch [19/100]\n  Train -> Loss: 9.7848 (Cls: 0.8330, BBox: 0.8952), Acc: 75.35%\n  Valid -> Loss: 780.5704 (Cls: 771.4988, BBox: 0.9072), Acc: 75.74%\nEpoch [20/100]\n  Train -> Loss: 9.7200 (Cls: 0.7961, BBox: 0.8924), Acc: 75.72%\n  Valid -> Loss: 128.6666 (Cls: 119.6031, BBox: 0.9063), Acc: 77.43%\nEpoch [21/100]\n  Train -> Loss: 9.5540 (Cls: 0.7281, BBox: 0.8826), Acc: 75.98%\n  Valid -> Loss: 82.9651 (Cls: 73.9973, BBox: 0.8968), Acc: 78.06%\nEpoch [22/100]\n  Train -> Loss: 9.4645 (Cls: 0.7062, BBox: 0.8758), Acc: 75.93%\n  Valid -> Loss: 83.0979 (Cls: 74.2107, BBox: 0.8887), Acc: 78.16%\nEpoch [23/100]\n  Train -> Loss: 9.5011 (Cls: 0.7339, BBox: 0.8767), Acc: 74.85%\n  Valid -> Loss: 25.9624 (Cls: 17.0685, BBox: 0.8894), Acc: 78.59%\nEpoch [24/100]\n  Train -> Loss: 9.4579 (Cls: 0.6625, BBox: 0.8795), Acc: 76.83%\n  Valid -> Loss: 132.4564 (Cls: 123.5790, BBox: 0.8877), Acc: 77.74%\nEpoch [25/100]\n  Train -> Loss: 9.4589 (Cls: 0.6537, BBox: 0.8805), Acc: 77.38%\n  Valid -> Loss: 63.5657 (Cls: 54.6895, BBox: 0.8876), Acc: 77.95%\nEpoch [26/100]\n  Train -> Loss: 9.4362 (Cls: 0.6293, BBox: 0.8807), Acc: 77.43%\n  Valid -> Loss: 44.0477 (Cls: 35.2227, BBox: 0.8825), Acc: 78.48%\nEpoch [27/100]\n  Train -> Loss: 9.3960 (Cls: 0.6376, BBox: 0.8758), Acc: 76.70%\n  Valid -> Loss: 14.9611 (Cls: 6.2138, BBox: 0.8747), Acc: 78.69%\nEpoch [28/100]\n  Train -> Loss: 9.3832 (Cls: 0.6226, BBox: 0.8761), Acc: 76.91%\n  Valid -> Loss: 21.2980 (Cls: 12.6020, BBox: 0.8696), Acc: 77.95%\nEpoch [29/100]\n  Train -> Loss: 9.3759 (Cls: 0.6229, BBox: 0.8753), Acc: 77.33%\n  Valid -> Loss: 11.9853 (Cls: 3.2839, BBox: 0.8701), Acc: 78.38%\nEpoch [30/100]\n  Train -> Loss: 9.4262 (Cls: 0.6565, BBox: 0.8770), Acc: 76.46%\n  Valid -> Loss: 9.3369 (Cls: 0.6388, BBox: 0.8698), Acc: 78.69%\nEpoch [31/100]\n  Train -> Loss: 9.4010 (Cls: 0.6150, BBox: 0.8786), Acc: 76.85%\n  Valid -> Loss: 9.5982 (Cls: 0.8452, BBox: 0.8753), Acc: 78.90%\nEpoch [32/100]\n  Train -> Loss: 9.3966 (Cls: 0.6452, BBox: 0.8751), Acc: 76.70%\n  Valid -> Loss: 79.1488 (Cls: 70.3042, BBox: 0.8845), Acc: 79.01%\nEpoch [33/100]\n  Train -> Loss: 9.3337 (Cls: 0.6374, BBox: 0.8696), Acc: 77.14%\n  Valid -> Loss: 71.4859 (Cls: 62.6705, BBox: 0.8815), Acc: 78.59%\nEpoch [34/100]\n  Train -> Loss: 9.2891 (Cls: 0.6258, BBox: 0.8663), Acc: 77.46%\n  Valid -> Loss: 97.3272 (Cls: 88.4965, BBox: 0.8831), Acc: 78.90%\nEpoch [35/100]\n  Train -> Loss: 9.2511 (Cls: 0.5916, BBox: 0.8659), Acc: 77.94%\n  Valid -> Loss: 11.8850 (Cls: 3.1972, BBox: 0.8688), Acc: 79.11%\nEpoch [36/100]\n  Train -> Loss: 9.2541 (Cls: 0.5867, BBox: 0.8667), Acc: 78.49%\n  Valid -> Loss: 42.3172 (Cls: 33.5275, BBox: 0.8790), Acc: 79.43%\nEpoch [37/100]\n  Train -> Loss: 9.2304 (Cls: 0.5932, BBox: 0.8637), Acc: 77.62%\n  Valid -> Loss: 99.0550 (Cls: 90.2776, BBox: 0.8777), Acc: 79.43%\nEpoch [38/100]\n  Train -> Loss: 9.2257 (Cls: 0.5844, BBox: 0.8641), Acc: 78.23%\n  Valid -> Loss: 11.1483 (Cls: 2.4762, BBox: 0.8672), Acc: 79.32%\nEpoch [39/100]\n  Train -> Loss: 9.2080 (Cls: 0.5809, BBox: 0.8627), Acc: 78.09%\n  Valid -> Loss: 9.2824 (Cls: 0.5796, BBox: 0.8703), Acc: 79.43%\nEpoch [40/100]\n  Train -> Loss: 9.2142 (Cls: 0.5927, BBox: 0.8622), Acc: 78.23%\n  Valid -> Loss: 40.7413 (Cls: 32.0006, BBox: 0.8741), Acc: 80.06%\nEpoch [41/100]\n  Train -> Loss: 9.2384 (Cls: 0.5989, BBox: 0.8640), Acc: 78.78%\n  Valid -> Loss: 9.2904 (Cls: 0.6178, BBox: 0.8673), Acc: 80.17%\nEpoch [42/100]\n  Train -> Loss: 9.1856 (Cls: 0.5670, BBox: 0.8619), Acc: 78.60%\n  Valid -> Loss: 9.2423 (Cls: 0.5982, BBox: 0.8644), Acc: 80.27%\nEpoch [43/100]\n  Train -> Loss: 9.1998 (Cls: 0.5777, BBox: 0.8622), Acc: 78.52%\n  Valid -> Loss: 16.2069 (Cls: 7.4971, BBox: 0.8710), Acc: 80.91%\nEpoch [44/100]\n  Train -> Loss: 9.1741 (Cls: 0.5712, BBox: 0.8603), Acc: 78.73%\n  Valid -> Loss: 9.2716 (Cls: 0.6325, BBox: 0.8639), Acc: 80.80%\nEpoch [45/100]\n  Train -> Loss: 9.1556 (Cls: 0.5566, BBox: 0.8599), Acc: 80.44%\n  Valid -> Loss: 9.6879 (Cls: 1.0474, BBox: 0.8641), Acc: 80.91%\nEpoch [46/100]\n  Train -> Loss: 9.1617 (Cls: 0.5480, BBox: 0.8614), Acc: 79.47%\n  Valid -> Loss: 10.4547 (Cls: 1.8335, BBox: 0.8621), Acc: 80.70%\nEpoch [47/100]\n  Train -> Loss: 9.1511 (Cls: 0.5618, BBox: 0.8589), Acc: 79.26%\n  Valid -> Loss: 37.6615 (Cls: 28.9989, BBox: 0.8663), Acc: 81.01%\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def plot_training_history(history_dict):\n    epochs_range = range(1, len(history_dict['train_loss']) + 1)\n\n    plt.figure(figsize=(18, 6))\n\n    # Plot Total Loss\n    plt.subplot(1, 3, 1)\n    plt.plot(epochs_range, history_dict['train_loss'], 'bo-', label='Training Total Loss')\n    plt.plot(epochs_range, history_dict['val_loss'], 'ro-', label='Validation Total Loss')\n    plt.title('Total Loss (Combined)', fontsize=14)\n    plt.xlabel('Epochs', fontsize=12)\n    plt.ylabel('Loss', fontsize=12)\n    plt.legend()\n    plt.grid(True, linestyle='--')\n\n    # Plot Accuracy\n    plt.subplot(1, 3, 2)\n    plt.plot(epochs_range, history_dict['train_acc'], 'bo-', label='Training Accuracy')\n    plt.plot(epochs_range, history_dict['val_acc'], 'ro-', label='Validation Accuracy')\n    plt.title('Classification Accuracy', fontsize=14)\n    plt.xlabel('Epochs', fontsize=12)\n    plt.ylabel('Accuracy (%)', fontsize=12)\n    plt.legend()\n    plt.grid(True, linestyle='--')\n\n    # Plot Individual Losses (Optional, can be expanded)\n    plt.subplot(1, 3, 3)\n    plt.plot(epochs_range, history_dict['train_class_loss'], 'bs-', label='Training Class Loss')\n    plt.plot(epochs_range, history_dict['val_class_loss'], 'rs-', label='Validation Class Loss')\n    plt.plot(epochs_range, history_dict['train_bbox_loss'], 'g^-', label='Training BBox Loss')\n    plt.plot(epochs_range, history_dict['val_bbox_loss'], 'c^-', label='Validation BBox Loss')\n    plt.title('Component Losses', fontsize=14)\n    plt.xlabel('Epochs', fontsize=12)\n    plt.ylabel('Loss', fontsize=12)\n    plt.legend()\n    plt.grid(True, linestyle='--')\n    \n    plt.tight_layout()\n    plt.savefig(\"training.jpg\")\n    plt.show()\n\n# Plot the collected history after training completes\nif history['train_loss']: # Check if training actually ran\n    plot_training_history(history)\nelse:\n    print(\"No training history to plot.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:12:45.906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.patches as patches # For drawing bounding boxes\n\n# Helper function to denormalize image tensor for visualization\ndef denormalize_image(tensor, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n    \"\"\"Denormalizes an image tensor.\"\"\"\n    if not torch.is_tensor(tensor):\n        tensor = torch.tensor(tensor)\n\n    # Clone the tensor to avoid modifying the original\n    tensor = tensor.clone()\n\n    for t, m, s in zip(tensor, mean, std):\n        t.mul_(s).add_(m) # Multiply by std and add mean\n    return tensor\n\n\ndef convert_norm_bbox_to_pixel(bbox_norm, img_width, img_height):\n    \"\"\"Converts a normalized bounding box to pixel coordinates.\"\"\"\n    if isinstance(bbox_norm, torch.Tensor):\n        bbox_norm = bbox_norm.cpu().numpy()\n\n    x_center, y_center, norm_w, norm_h = bbox_norm\n    \n    abs_w = norm_w * img_width\n    abs_h = norm_h * img_height\n    \n    x_min = int(x_center * img_width - abs_w / 2)\n    y_min = int(y_center * img_height - abs_h / 2)\n    x_max = int(x_center * img_width + abs_w / 2)\n    y_max = int(y_center * img_height + abs_h / 2)\n    \n    # Clip coordinates to be within image dimensions\n    x_min = max(0, x_min)\n    y_min = max(0, y_min)\n    x_max = min(img_width, x_max)\n    y_max = min(img_height, y_max)\n    \n    return [x_min, y_min, x_max, y_max]\n\n\ndef calculate_iou(boxA_pixel, boxB_pixel):\n    \"\"\"Calculates IoU between two bounding boxes in pixel coordinates [x_min, y_min, x_max, y_max].\"\"\"\n    # Determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA_pixel[0], boxB_pixel[0])\n    yA = max(boxA_pixel[1], boxB_pixel[1])\n    xB = min(boxA_pixel[2], boxB_pixel[2])\n    yB = min(boxA_pixel[3], boxB_pixel[3])\n\n    # Compute the area of intersection rectangle\n    interArea = max(0, xB - xA) * max(0, yB - yA)\n\n    # Compute the area of both the prediction and ground-truth rectangles\n    boxAArea = (boxA_pixel[2] - boxA_pixel[0]) * (boxA_pixel[3] - boxA_pixel[1])\n    boxBArea = (boxB_pixel[2] - boxB_pixel[0]) * (boxB_pixel[3] - boxB_pixel[1])\n\n    # Compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the intersection area\n    denominator = float(boxAArea + boxBArea - interArea)\n    iou = interArea / denominator if denominator > 0 else 0.0\n    \n    return iou","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:12:45.906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport numpy as np\n\ndef generate_evaluation_report(trained_model, data_loader, current_label_encoder, current_device, image_input_size):\n    \"\"\" \n    Generates and prints a classification report, confusion matrix, mean IoU, and percentage of IoUs > 0.5\n    Args: \n        trained_model: Trained model \n        data_loader: DataLoader for the evaluation set \n        current_label_encoder: fitted LabelEncoder for class names \n        current_device: the device to run the model on \n        image_input_size: Tuple (height, width) of the input images to the model \n    \"\"\"\n\n    trained_model.eval() \n\n    all_true_labels_list = []\n    all_pred_labels_list = []\n    iou_scores_list = []\n\n    # Get image dimensions for bbox conversion \n    img_h, img_w = image_input_size \n\n    with torch.no_grad(): \n        for batch_images, batch_true_labels_idx, batch_true_bboxes_norm in data_loader: \n            batch_images = batch_images.to(current_device)\n\n            # Get model predictions\n            batch_class_logits, batch_pred_bboxes_norm = trained_model(batch_images)\n\n            # Get predicted class indices\n            batch_pred_labels_idx = torch.argmax(batch_class_logits, dim=1).cpu().numpy()\n            batch_true_labels_idx_np = batch_true_labels_idx.cpu().numpy()\n\n            # Store labels for classification report\n            all_true_labels_list.extend(batch_true_labels_idx_np)\n            all_pred_labels_list.extend(batch_pred_labels_idx)\n\n            # Process each sample in the batch for IoU calculation\n            for i in range(len(batch_true_labels_idx_np)):\n                true_label_idx = batch_true_labels_idx_np[i]\n                true_class_name_str = current_label_encoder.inverse_transform([true_label_idx])[0]\n\n                true_bbox_norm_sample = batch_true_bboxes_norm[i].cpu().numpy()\n                pred_bbox_norm_sample = batch_pred_bboxes_norm[i].cpu().numpy()\n\n                # Convert normalized bboxes to pixel coordinates\n                # Note: convert_norm_bbox_to_pixel expects (width, height) order for dimensions\n                true_bbox_pixel = convert_norm_bbox_to_pixel(true_bbox_norm_sample, img_w, img_h)\n                pred_bbox_pixel = convert_norm_bbox_to_pixel(pred_bbox_norm_sample, img_w, img_h)\n\n                iou = calculate_iou(true_bbox_pixel, pred_bbox_pixel)\n                iou_scores_list.append(iou)\n\n    print(\"\\n === DETAILED EVALUATION REPORT === \")\n    \n    # 1. Classification Report\n    print(\"\\n Classification Report:\")\n    if len(all_true_labels_list) > 0:\n        class_names_for_report = current_label_encoder.classes_\n        report = classification_report(all_true_labels_list, all_pred_labels_list, \n                                       target_names=class_names_for_report, zero_division=0)\n        print(report)\n    else:\n        print(\"No samples processed, cannot generate classification report.\")\n\n    # 1.5 Confusion Matrix\n    print(\"\\n Confusion Matrix:\")\n    if len(all_true_labels_list) > 0:\n        cm = confusion_matrix(all_true_labels_list, all_pred_labels_list)\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                    xticklabels=current_label_encoder.classes_,\n                    yticklabels=current_label_encoder.classes_)\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.title('Confusion Matrix')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(\"No samples processed, cannot generate confusion matrix.\")\n\n    # 2. Bounding Box Metrics\n    print(\"\\n Bounding Box Metrics:\")\n    if iou_scores_list:\n        mean_iou = np.mean(iou_scores_list)\n        iou_threshold = 0.5\n        good_iou_count = np.sum(np.array(iou_scores_list) > iou_threshold)\n        percentage_good_iou = (good_iou_count / len(iou_scores_list)) * 100 if len(iou_scores_list) > 0 else 0.0\n\n        print(f\"  Number of tumor samples considered for IoU: {len(iou_scores_list)}\")\n        print(f\"  Mean IoU (mIoU): {mean_iou:.4f}\")\n        print(f\"  Percentage of samples with IoU > {iou_threshold}: {percentage_good_iou:.2f}%\")\n    else:\n        print(\"  No tumor samples found or processed for IoU calculation (e.g., all samples were 'No Tumor' or no valid bboxes).\")\n\n    print(\"======================================\\n\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:12:45.906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main function to visualize predictions\ndef visualize_predictions(trained_model, data_loader, current_label_encoder, current_device, num_samples=5):\n    \"\"\"\n    Visualizes model predictions on a few samples from the data_loader.\n    Displays image, true/predicted classes, true/predicted bounding boxes, and IoU.\n    \"\"\"\n    trained_model.eval() # Set model to evaluation mode\n    \n            \n    samples_shown = 0\n    with torch.no_grad(): # No need to calculate gradients\n        for batch_images, batch_true_labels_idx, batch_true_bboxes_norm in data_loader:\n            if samples_shown >= num_samples:\n                break\n\n            batch_images = batch_images.to(current_device)\n            # No need to move labels and bboxes to device for this visualization part if processed on CPU later\n\n            # Get model predictions for the batch\n            batch_class_logits, batch_pred_bboxes_norm = trained_model(batch_images)\n            \n            # Process each image in the batch\n            for i in range(batch_images.size(0)):\n                if samples_shown >= num_samples:\n                    break\n\n                img_tensor = batch_images[i].cpu()\n                true_label_idx = batch_true_labels_idx[i].item()\n                true_bbox_norm = batch_true_bboxes_norm[i].cpu().numpy() # Should be [xc, yc, w, h]\n                \n                pred_label_idx = torch.argmax(batch_class_logits[i]).cpu().item()\n                pred_bbox_norm = batch_pred_bboxes_norm[i].cpu().numpy() # Should be [xc, yc, w, h]\n\n                # Denormalize image for display\n                img_to_show = denormalize_image(img_tensor)\n                img_to_show_np = img_to_show.permute(1, 2, 0).numpy().clip(0, 1) # HWC, clip to [0,1]\n                img_h, img_w = img_to_show_np.shape[:2]\n\n                # Get class names\n                true_class_name = current_label_encoder.inverse_transform([true_label_idx])[0]\n                pred_class_name = current_label_encoder.inverse_transform([pred_label_idx])[0]\n\n                # Convert bounding boxes to pixel coordinates [x_min, y_min, x_max, y_max]\n                true_bbox_pixel = convert_norm_bbox_to_pixel(true_bbox_norm, img_w, img_h)\n                pred_bbox_pixel = convert_norm_bbox_to_pixel(pred_bbox_norm, img_w, img_h)\n                \n                # Calculate IoU\n                # Only calculate IoU if the true class is not \"No Tumor\" (or a similar negative class)\n                # because the \"true\" bbox for \"No Tumor\" is often a dummy [0,0,0,0]\n                iou = 0.0\n                iou = calculate_iou(true_bbox_pixel, pred_bbox_pixel)\n               \n                # Plotting\n                fig, ax = plt.subplots(1, figsize=(8, 8))\n                ax.imshow(img_to_show_np)\n                ax.axis('off') # Hide axes ticks\n\n                title_str = f\"True: {true_class_name} | Pred: {pred_class_name}\"\n\n                title_str += f\"\\nIoU: {iou:.2f}\"\n                \n                # Draw True Bounding Box (Green)\n                # Only draw if it's not a \"No Tumor\" class, as its bbox is likely a dummy [0,0,0,0]\n\n                true_x_min, true_y_min, true_x_max, true_y_max = true_bbox_pixel\n                true_rect_w = true_x_max - true_x_min\n                true_rect_h = true_y_max - true_y_min\n                rect_true = patches.Rectangle((true_x_min, true_y_min), true_rect_w, true_rect_h,\n                                                  linewidth=2, edgecolor='lime', facecolor='none', label=f'True: {true_class_name}')\n                ax.add_patch(rect_true)\n                ax.text(true_x_min, true_y_min - 10, f'True: {true_class_name}', color='lime', fontsize=10,\n                            bbox=dict(facecolor='black', alpha=0.5, pad=1))\n\n\n               \n                pred_x_min, pred_y_min, pred_x_max, pred_y_max = pred_bbox_pixel\n                pred_rect_w = pred_x_max - pred_x_min\n                pred_rect_h = pred_y_max - pred_y_min\n                rect_pred = patches.Rectangle((pred_x_min, pred_y_min), pred_rect_w, pred_rect_h,\n                                                  linewidth=2, edgecolor='red', facecolor='none', label=f'Pred: {pred_class_name}')\n                ax.add_patch(rect_pred)\n                    \n                text_y_pos = pred_y_min - 10 if pred_y_min - 10 > 10 else pred_y_max + 10 # Adjust text position\n                pred_text = f'Pred: {pred_class_name}'\n                 # Add IoU if at least one box is meaningful\n                pred_text += f'\\nIoU: {iou:.2f}'\n                ax.text(pred_x_min, text_y_pos, pred_text, color='red', fontsize=10,\n                            bbox=dict(facecolor='black', alpha=0.5, pad=1))\n              \n                ax.text(10, 10, f'Pred: {pred_class_name} (Missed Tumor)', color='orange', fontsize=10,\n                            bbox=dict(facecolor='black', alpha=0.5, pad=1))\n\n\n                plt.title(title_str, fontsize=12)\n                # plt.legend() # Can be a bit cluttered with text directly on image\n                plt.show()\n                \n                samples_shown += 1\n                \n            if samples_shown >= num_samples: # Check again after inner loop\n                break","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:12:45.906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" visualize_predictions(model, test_loader, label_encoder, DEVICE, num_samples=20)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:12:45.906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_evaluation_report(model, test_loader, label_encoder, DEVICE, IMG_SIZE)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-02T07:12:45.906Z"}},"outputs":[],"execution_count":null}]}